{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd   \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier,  GradientBoostingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import log_loss\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[credit.notnull()]\n",
    "X_test = data[credit.isnull()]\n",
    "y_train = credit.dropna().astype(int)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter 별로 search할 범위를 설정. \n",
    "bayesian_params = {\n",
    "    'max_depth': (1, 5), \n",
    "    'num_leaves': (24, 64), \n",
    "    'min_child_samples': (10, 200), \n",
    "    'min_child_weight':(1, 50),\n",
    "    'subsample':(0.5, 1.0),\n",
    "    'colsample_bytree': (0.5, 1.0),\n",
    "    'max_bin':(10, 500),\n",
    "    'reg_lambda':(0.001, 10),\n",
    "    'reg_alpha': (0.001, 10) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_roc_eval(max_depth, num_leaves, min_child_samples, min_child_weight, subsample, \n",
    "                colsample_bytree,max_bin, reg_lambda, reg_alpha):\n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        \"n_estimators\":500, \"learning_rate\":0.02,\n",
    "        'max_depth': int(round(max_depth)), #  호출 시 실수형 값이 들어오므로 정수형 하이퍼 파라미터는 정수형으로 변경 \n",
    "        'num_leaves': int(round(num_leaves)), \n",
    "        'min_child_samples': int(round(min_child_samples)),\n",
    "        'min_child_weight': int(round(min_child_weight)),\n",
    "        'subsample': max(min(subsample, 1), 0), \n",
    "        'colsample_bytree': max(min(colsample_bytree, 1), 0),\n",
    "        'max_bin':  max(int(round(max_bin)),10),\n",
    "        'reg_lambda': max(reg_lambda,0),\n",
    "        'reg_alpha': max(reg_alpha, 0)\n",
    "    }\n",
    "    lgb_model = LGBMClassifier(**params)\n",
    "    lgb_model.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= 'logloss', verbose= 100, \n",
    "                early_stopping_rounds= 100)\n",
    "    valid_proba = lgb_model.predict_proba(valid_x)\n",
    "    logLoss = log_loss(valid_y, valid_proba)\n",
    "    \n",
    "    return -logLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BayesianOptimization객체를 수행할 함수와 search할 parameter 범위를 설정하여 생성. \n",
    "lgbBO = BayesianOptimization(lgb_roc_eval,bayesian_params , random_state=0)\n",
    "# 함수 반환값이 최대가 되는 입력값 유추를 위한 iteration 수행. \n",
    "lgbBO.maximize(init_points=5, n_iter=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary에 있는 target값을 모두 추출\n",
    "target_list = []\n",
    "for result in lgbBO.res:\n",
    "    target = result['target']\n",
    "    target_list.append(target)\n",
    "print(target_list)\n",
    "# 가장 큰 target 값을 가지는 순번(index)를 추출\n",
    "print('maximum target index:', np.argmax(np.array(target_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 큰 target값을 가지는 index값을 기준으로 res에서 해당 parameter 추출. \n",
    "max_dict = lgbBO.res[np.argmax(np.array(target_list))]\n",
    "max_params = max_dict['params']\n",
    "print(max_dict['target'])\n",
    "print(max_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'colsample_bytree': round(max_params['colsample_bytree'], 3),\n",
    " 'max_bin': int(max_params['max_bin']),\n",
    " 'max_depth': int(max_params['max_depth']),\n",
    " 'min_child_samples': int(max_params['min_child_samples']),\n",
    " 'min_child_weight': int(max_params['min_child_weight']),\n",
    " 'num_leaves': int(max_params['num_leaves']),\n",
    " 'reg_alpha': round(max_params['reg_alpha'], 3),\n",
    " 'reg_lambda': round(max_params['reg_lambda'], 3),\n",
    " 'subsample': round(max_params['subsample'], 3),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_model = LGBMClassifier(**params)\n",
    "lgb_model.fit(X_train, y_train, verbose= 100)\n",
    "pred_lgb = lgb_model.predict_proba(X_test)\n",
    "pred_lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "rf.fit(X_train , y_train)\n",
    "pred_rf = rf.predict_proba(X_test)\n",
    "pred_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#n_estimators : 생성할 tree의 개수\n",
    "\n",
    "#max_features : 최대 선택할 변수의 수\n",
    "#max_features 값을 크게 하면 random forest의 tree들은 같은 변수를 고려하므로 \n",
    "#tree들이 비슷해지고 가장 두드러진 변수를 이용해 데이터에 잘 맞춰짐\n",
    "\n",
    "#max_features를 낮추면 \n",
    "#random forest tree들은 많이 달라지고 각 tree는 데이터에 맞추기 위해 tree의 깊이가 깊어집니다.\n",
    "\n",
    "#max_depth : 랜덤포레스트 안에 있는 각 의사결정나무의 깊이를 설정. \n",
    "# 트리가 깊어질수록 더 잘게 분류를 시키므로 일반적으론 정확도가 높아진다. 하지만 오버피팅의 위험이 존재\n",
    "\n",
    "params = {\n",
    "    'n_estimators':[100, 300, 500]\n",
    "}\n",
    "\n",
    "#Revenue의 T, F의 비율이 8,5 : 1.5로 치우처져있으므로 그 비율에 맞게 sampling하는 StratifiedKFold사용\n",
    "cv = StratifiedKFold(n_splits=5, random_state=0)\n",
    "\n",
    "# 랜덤포레스트 객체 생성 \n",
    "rf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "\n",
    "# f1 스코어 기준으로 GridSearchCV 수행\n",
    "# refit = True :  best estimator로 자동으로 수정됨\n",
    "# n_jobs = -1 : 모든 cpu의 코어를 사용\n",
    "grid_cv = GridSearchCV(rf , param_grid=params , cv=cv, scoring=\"logloss\", n_jobs=-1, refit = True)\n",
    "\n",
    "# 모델 학습\n",
    "grid_cv.fit(X_train , y_train)\n",
    "\n",
    "print('최적 하이퍼 파라미터:\\n', grid_cv.best_params_)\n",
    "print('최고 f1: {0:.4f}'.format(grid_cv.best_score_))\n",
    "\n",
    "# 최적의 파라미터로 모델 생성 및 예측\n",
    "model = grid_cv.best_estimator_\n",
    "pred_rf = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print('f1: {0:.4f}'.format(metrics.f1_score(y_test , pred_rf)))\n",
    "print('accuracy: {0:.4f}'.format(metrics.accuracy_score(y_test , pred_rf)))\n",
    "print('precision: {0:.4f}'.format(metrics.precision_score(y_test , pred_rf)))\n",
    "print('recall: {0:.4f}'.format(metrics.recall_score(y_test , pred_rf)))\n",
    "\n",
    "# Confusion Matrix\n",
    "metrics.plot_confusion_matrix(model, X_test, y_test, cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = (pred_lgb + pred_rf)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=pd.read_csv('sample_submission.csv')\n",
    "submission[['0','1','2']] = pd.DataFrame(pred)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
